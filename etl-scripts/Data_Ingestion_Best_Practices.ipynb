{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6f347d0d-455e-42e8-8005-3e67e66313f2",
   "metadata": {},
   "source": [
    "# **<span style=\"color: cornflowerblue;\">Data Ingestion - Best Practices</span>**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68d5d860-3367-4a51-a2f5-2805a9b05119",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### Table of Contents"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c26f1710-6325-456c-9d60-4876f15a02bd",
   "metadata": {},
   "source": [
    "<span style=\"color:dodgerblue;font-size:20px;font-weight:bold;\">Detailed ETL Process Outline</span>\n",
    "\n",
    "> <span style=\"color:blue;font-size:18px;font-weight:bold;\">1. Introduction to ETL</span>\n",
    ">> <span style=\"color:cornflowerblue;\">1.1 Explanation of the ETL process</span>\n",
    ">>> - a. What is ETL?\n",
    ">>> - b. Different stages in ETL\n",
    ">>> - c. Use cases of ETL\n",
    "\n",
    ">> <span style=\"color:cornflowerblue;\">1.2 Importance of ETL</span>\n",
    ">>> - a. Why is ETL important?\n",
    ">>> - b. What are the advantages of using ETL?\n",
    "\n",
    ">> <span style=\"color:cornflowerblue;\">1.3 Use cases</span>\n",
    ">>> - a. Real-world use cases of ETL\n",
    ">>> - b. ETL in the automotive industry\n",
    "\n",
    "> <span style=\"color:blue;font-size:18px;font-weight:bold;\">2. Introduction to AWS S3</span>\n",
    ">> <span style=\"color:cornflowerblue;\">2.1 What is AWS S3?</span>\n",
    ">>> - a. Brief about AWS S3\n",
    ">>> - b. Key features of AWS S3\n",
    "\n",
    ">> <span style=\"color:cornflowerblue;\">2.2 Use cases of S3 in data processing</span>\n",
    ">>> - a. How is S3 used in data processing?\n",
    ">>> - b. Advantages of using S3 for data storage\n",
    "\n",
    ">> <span style=\"color:cornflowerblue;\">2.3 Working with S3</span>\n",
    ">>> - a. Basic operations with S3\n",
    ">>> - b. Uploading and downloading data from S3\n",
    "\n",
    "> <span style=\"color:blue;font-size:18px;font-weight:bold;\">3. Setting up the PySpark Environment</span>\n",
    ">> <span style=\"color:cornflowerblue;\">3.1 What is PySpark?</span>\n",
    ">>> - a. Introduction to PySpark\n",
    ">>> - b. Features of PySpark\n",
    "\n",
    ">> <span style=\"color:cornflowerblue;\">3.2 Why use PySpark for ETL?</span>\n",
    ">>> - a. Advantages of using PySpark\n",
    ">>> - b. PySpark in ETL\n",
    "\n",
    ">> <span style=\"color:cornflowerblue;\">3.3 Setting up the Spark Session</span>\n",
    ">>> - a. What is a Spark Session?\n",
    ">>> - b. How to create a Spark Session?\n",
    "\n",
    "> <span style=\"color:blue;font-size:18px;font-weight:bold;\">4. Loading Data from S3 to PySpark DataFrame</span>\n",
    ">> <span style=\"color:cornflowerblue;\">4.1 Importing the required libraries</span>\n",
    ">>> - a. What libraries are required?\n",
    ">>> - b. How to import these libraries?\n",
    "\n",
    ">> <span style=\"color:cornflowerblue;\">4.2 Reading data from S3</span>\n",
    ">>> - a. How to read data from S3 using PySpark?\n",
    ">>> - b. What are the options available while reading the data?\n",
    "\n",
    ">> <span style=\"color:cornflowerblue;\">4.3 Data Inspection</span>\n",
    ">>> - a. Checking the data schema\n",
    ">>> - b. Previewing the data\n",
    "\n",
    "> <span style=\"color:blue;font-size:18px;font-weight:bold;\">5. Transforming Data with PySpark</span>\n",
    ">> <span style=\"color:cornflowerblue;\">5.1 Data Cleaning</span>\n",
    ">>> - a. Handling missing values\n",
    ">>> - b. Handling outliers\n",
    ">>> - c. String manipulation\n",
    "\n",
    ">> <span style=\"color:cornflowerblue;\">5.2 Data Transformation</span>\n",
    ">>> - a. Feature extraction\n",
    ">>> - b. Feature scaling\n",
    ">>> - c. Encoding categorical variables\n",
    "\n",
    ">> <span style=\"color:cornflowerblue;\">5.3 Data Aggregation</span>\n",
    ">>> - a. GroupBy operations\n",
    ">>> - b. Aggregation functions\n",
    ">>> - c. Pivot and unpivot operations\n",
    "\n",
    "> <span style=\"color:blue;font-size:18px;font-weight:bold;\">6. Loading Data back into S3</span>\n",
    ">> <span style=\"color:cornflowerblue;\">6.1 Formatting data for export</span>\n",
    ">>> - a. Selecting required columns\n",
    ">>> - b. Converting data types\n",
    "\n",
    ">> <span style=\"color:cornflowerblue;\">6.2 Writing data to S3</span>\n",
    ">>> - a. Setting write options\n",
    ">>> - b. Executing write operation\n",
    "\n",
    ">> <span style=\"color:cornflowerblue;\">6.3 Verifying the upload</span>\n",
    ">>> - a. Checking S3 bucket\n",
    ">>> - b. Reading data from S3\n",
    "\n",
    "> <span style=\"color:blue;font-size:18px;font-weight:bold;\">7. Streaming Data and Real-time ETL</span>\n",
    ">> <span style=\"color:cornflowerblue;\">7.1 Introduction to Streaming Data</span>\n",
    ">>> - a. What is streaming data?\n",
    ">>> - b. Use cases of streaming data\n",
    "\n",
    ">> <span style=\"color:cornflowerblue;\">7.2 Real-time ETL with PySpark</span>\n",
    ">>> - a. Setting up the streaming environment\n",
    ">>> - b. Transforming streaming data\n",
    ">>> - c. Loading streaming data\n",
    "\n",
    ">> <span style=\"color:cornflowerblue;\">7.3 Example with GM vehicle data</span>\n",
    ">>> - a. What kind of data GM provides?\n",
    ">>> - b. Real-time ETL with GM data\n",
    "\n",
    "> <span style=\"color:blue;font-size:18px;font-weight:bold;\">8. Data Analysis and Visualization with PySpark</span>\n",
    ">> <span style=\"color:cornflowerblue;\">8.1 Basic Data Analysis</span>\n",
    ">>> - a. Descriptive statistics\n",
    ">>> - b. Correlation and covariance\n",
    ">>> - c. Cross-tabulation and pivot tables\n",
    "\n",
    ">> <span style=\"color:cornflowerblue;\">8.2 Data Visualization</span>\n",
    ">>> - a. PySpark integration with Matplotlib\n",
    ">>> - b. Plotting distributions\n",
    ">>> - c. Visualizing correlations\n",
    "\n",
    ">> <span style=\"color:cornflowerblue;\">8.3 Advanced Analysis (optional)</span>\n",
    ">>> - a. Time series analysis\n",
    ">>> - b. Text analysis (if applicable)\n",
    "\n",
    "> <span style=\"color:blue;font-size:18px;font-weight:bold;\">9. Machine Learning with PySpark</span>\n",
    ">> <span style=\"color:cornflowerblue;\">9.1 Introduction to MLlib</span>\n",
    ">>> - a. MLlib Overview\n",
    ">>> - b. Data preparation for MLlib\n",
    "\n",
    ">> <span style=\"color:cornflowerblue;\">9.2 Building Machine Learning Models</span>\n",
    ">>> - a. Supervised learning\n",
    ">>> - b. Unsupervised learning\n",
    ">>> - c. Model evaluation and tuning\n",
    "\n",
    ">> <span style=\"color:cornflowerblue;\">9.3 Example with GM data (optional)</span>\n",
    ">>> - a. Predictive maintenance\n",
    ">>> - b. Customer behavior prediction\n",
    "\n",
    "> <span style=\"color:blue;font-size:18px;font-weight:bold;\">10. Final Thoughts and Best Practices</span>\n",
    ">> <span style=\"color:cornflowerblue;\">10.1 Recap and Key Takeaways</span>\n",
    ">>> - a. Importance of ETL\n",
    ">>> - b. Power of PySpark\n",
    ">>> - c. Importance of data cleanliness\n",
    "\n",
    ">> <span style=\"color:cornflowerblue;\">10.2 Best Practices</span>\n",
    ">>> - a. ETL best practices\n",
    ">>> - b. PySpark coding best practices\n",
    ">>> - c. Data analysis best practices\n",
    "\n",
    ">> <span style=\"color:cornflowerblue;\">10.3 Future Exploration</span>\n",
    ">>> - a. Other PySpark functionalities\n",
    ">>> - b. Other big data technologies\n",
    ">>> - c. Advanced Machine Learning concepts\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5d7902a-e7da-437b-9dc2-a1864467e9c0",
   "metadata": {},
   "source": [
    "### Ingesting Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6aafaf73-2356-4ebf-9092-3ef0010a5de8",
   "metadata": {},
   "source": [
    "<b><dt style=\"color: cornflowerblue;\">1. Add Alerts at source for data issues</dt></b>\n",
    "<dd>\n",
    "Adding alerts in the source data will save a lot of time trying to debug issues downstream. Basic data quality checks like null column, duplicate records, and invalid data can be checked before loading the data into the repository. If the checks fail, alerts must be triggered for the source team to fix. The faulty records can be discarded and logged.\n",
    "</dd>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bdd62e8-300c-4ade-b25c-8a623e97e7af",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, when, count, isnull\n",
    "\n",
    "spark = SparkSession.builder.getOrCreate()\n",
    "\n",
    "# Load your data\n",
    "df = spark.read.csv('your_data.csv', header=True)\n",
    "\n",
    "# Check for nulls\n",
    "null_counts = df.select([count(when(isnull(c), c)).alias(c) for c in df.columns])\n",
    "null_counts.show()\n",
    "\n",
    "# Check for duplicates\n",
    "if df.count() > df.distinct().count():\n",
    "    raise Exception(\"Data has duplicates\")\n",
    "\n",
    "# Check for invalid data\n",
    "if df.filter(col('column_name') < 0).count() > 0: # Replace 'column_name' with actual column\n",
    "    raise Exception(\"Data has invalid records\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f06dbbea-390c-4fea-8b2b-78a6e2cc9592",
   "metadata": {},
   "source": [
    "<b><dt style=\"color: cornflowerblue;\">2. Keep a copy of all your raw data before applying the transformation</dt></b>\n",
    "<dd>\n",
    "The raw data layer must be read-only, and no one should have update access. This will serve as a backup in case of a failure in subsequent layers while trying to cleanse or transform the data.\n",
    "</dd>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2382521-2306-47e8-b92f-733bd73a3596",
   "metadata": {},
   "outputs": [],
   "source": [
    "# You can save a copy of your raw data to a specified location before applying transformations\n",
    "df.write.csv('/path/to/save/raw_data.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40b411b3-777b-42ae-a26c-551c3ba7ce8c",
   "metadata": {},
   "source": [
    "<b><dt style=\"color: cornflowerblue;\">3. Set expectations and timelines early, Data Ingestion isn’t easy</dt></b>\n",
    "<dd>\n",
    "Business leaders and project managers often either overestimate or underestimate the time needed for data ingestion. Data Ingestion can often be very complex, and ingestion pipelines need to have proper tests in place. Hence, it’s always good to set expectations of stakeholders on the timelines involved in building the pipeline and the time taken to load the data.\n",
    "</dd>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e258ad4-9fcb-4a12-bc8b-1f5ed5b5fa33",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import functions as F\n",
    "\n",
    "# Start timer\n",
    "start_time = F.current_timestamp()\n",
    "\n",
    "# Your data ingestion code here...\n",
    "\n",
    "# End timer\n",
    "end_time = F.current_timestamp()\n",
    "\n",
    "# Calculate elapsed time\n",
    "elapsed_time = (end_time.cast(\"long\") - start_time.cast(\"long\"))\n",
    "elapsed_time.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23ea852d-2f4d-4fb6-b8a1-4a07ec54d687",
   "metadata": {},
   "source": [
    "<b><dt style=\"color: cornflowerblue;\">4. Automate pipelines, use orchestration, set SLAs</dt></b>\n",
    "<dd>\n",
    "Data Ingestion pipelines should be automated, along with all the necessary dependencies. An orchestration tool can be used to synchronize different pipelines. Service Level Agreements (SLAs) must be set for each pipeline, which will allow monitoring teams to flag any pipelines that run longer than expected.\n",
    "</dd>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d1e7c29-556c-450f-ac33-c556e404aa10",
   "metadata": {},
   "source": [
    "<b><dt style=\"color: cornflowerblue;\">5. Data Ingestion Pipelines must be idempotent</dt></b>\n",
    "<dd>\n",
    "Idempotency is a critical characteristic for Data Ingestion Pipelines. It means that if you execute an operation multiple times, the result will not change after the initial execution. In the context of data integration, idempotence makes the data ingestion pipeline self-correcting and prevents duplicate records from being loaded. Strategies to achieve idempotency include Delete Insert, Upsert, Merge operations, and look up tasks.\n",
    "</dd>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a376ac54-ecc3-41f2-b5e0-0078b6ac891e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assuming df is your DataFrame and 'column_name' is the column you're working with\n",
    "df = df.dropDuplicates(['column_name'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79dd76b5-1a9f-4cd4-96d1-eddf617cba44",
   "metadata": {},
   "source": [
    "<b><dt style=\"color: cornflowerblue;\">6. Templatize, Reuse frameworks for development</dt></b>\n",
    "<dd>\n",
    "A lot of data ingestion pipelines are repetitive, so it’s important to create templates for pipeline development. If you create a reusable framework in your pipeline, the delivery effort can be significantly reduced. Increased velocity in ingesting new data will always be appreciated by the business.\n",
    "</dd>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef1505a3-3336-4f82-a7f1-1b047e980549",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_and_clean_data(file_path):\n",
    "    df = spark.read.csv(file_path, header=True)\n",
    "    df = df.dropDuplicates()\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0795d46-54e9-4917-8795-ff47e6be2a67",
   "metadata": {},
   "source": [
    "<b><dt style=\"color: cornflowerblue;\">7. Document your pipelines</dt></b>\n",
    "<dd>\n",
    "This isn't a code example, but it's still important. Be sure to add docstrings to your functions and classes, and comment your code where necessary to explain what it's doing. This makes it easier for others (and future you) to understand your code. For example:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7747306-c30f-4521-a3d9-da1c4cee39b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_and_clean_data(file_path):\n",
    "    \"\"\"\n",
    "    This function loads data from the given file path and removes duplicates.\n",
    "\n",
    "    Args:\n",
    "        file_path (str): The path to the data file.\n",
    "\n",
    "    Returns:\n",
    "        DataFrame: The cleaned data.\n",
    "    \"\"\"\n",
    "    df = spark.read.csv(file_path, header=True)\n",
    "    df = df.dropDuplicates()\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0db52ab-a1a1-482b-9456-3527030d65ab",
   "metadata": {},
   "source": [
    "<b><dt style=\"color: cornflowerblue;\">8. Proper Error Handling and Logging</dt></b>\n",
    "<dd>\n",
    "In any data pipeline, proper error handling and logging are crucial. They help to diagnose and understand the problems that might occur during the execution of pipelines."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0537ab75-467b-4ec4-b5f5-71f831ddb8d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "\n",
    "# Create a custom logger\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "# Log an error message\n",
    "try:\n",
    "    # Code that can raise an error...\n",
    "except Exception as e:\n",
    "    logger.error(\"An error occurred: \", exc_info=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba3d5c5c-f47c-4de3-9675-551624a685a8",
   "metadata": {},
   "source": [
    "</dd>\n",
    "<b><dt style=\"color: cornflowerblue;\">9. Monitoring and Auditing</dt></b>\n",
    "<dd>\n",
    "Monitoring and auditing are important aspects of maintaining data pipelines. Monitoring can help you keep an eye on the health of your pipelines, and auditing can help you keep track of changes and the flow of data through your pipelines.\n",
    "<i>Note: Detailed setup of monitoring and auditing would depend on the infrastructure and is beyond the scope of a code snippet.</i>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "746fa893-311f-427b-98dc-9b16137d1f68",
   "metadata": {},
   "source": [
    "<i>Note: Detailed setup of monitoring and auditing would depend on the infrastructure and is beyond the scope of a code snippet.</i>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "430a5644-8ef4-4a02-baa4-99add02b098c",
   "metadata": {},
   "source": [
    "</dd>\n",
    "<b><dt style=\"color: cornflowerblue;\">10. Unit Tests</dt></b>\n",
    "\n",
    "<dd>\n",
    "Just as with any other software components, data pipelines should be subjected to unit tests to ensure that all the individual units of your code are working as expected."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4eee6900-0000-45c4-8ae4-6895f225c272",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example using PyTest\n",
    "def test_load_and_clean_data():\n",
    "    df = load_and_clean_data('test_data.csv')\n",
    "    assert df is not None\n",
    "    assert df.count() > 0\n",
    "    assert df.distinct().count() == df.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9fc54ea-f48f-476f-825c-79b4c818def5",
   "metadata": {},
   "source": [
    "</dd>\n",
    "<b><dt style=\"color: cornflowerblue;\">11. Environment Parity</dt></b>\n",
    "<dd>\n",
    "Keeping all your environments - development, staging, production - similar helps in minimizing the number of bugs and issues during the production rollout."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5cc1c5e-9fcf-4f10-aa33-dba9726e4acc",
   "metadata": {},
   "source": [
    "**Note:** <i>Achieving environment parity often involves using containerization tools like Docker, and orchestration systems like Kubernetes, and is beyond the scope of a code snippet.</i>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "280f064f-1fff-4458-831e-33acc66e4309",
   "metadata": {},
   "source": [
    "</dd>\n",
    "<b><dt style=\"color: cornflowerblue;\">12. Data Validation</dt></b>\n",
    "<dd>\n",
    "Ensure that your data meets certain criteria by using the validation capabilities provided by PySpark."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e65d107-b80b-4399-b904-823bea72d7ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Validate that 'column_name' values are greater than 0\n",
    "df.filter(col('column_name') > 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79ec3a28-3d97-4b10-afed-5e6f33019455",
   "metadata": {},
   "source": [
    "<b><dt style=\"color: cornflowerblue;\">13. Scalability and Efficiency</dt></b>\n",
    "<dd>\n",
    "Always design your data pipelines with scalability in mind. As the volume of data grows, your pipelines should be able to handle the increased load efficiently."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b35f980-9c29-4539-bcb9-b8741151e410",
   "metadata": {},
   "source": [
    "**Note:** <i>Scalability is more of a design principle rather than a specific piece of code. Leveraging distributed computing frameworks like Spark would be an example of building for scalability.</i>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26ac9720-a06e-4c6d-bc08-f95f28f0cb73",
   "metadata": {},
   "source": [
    "</dd>\n",
    "<b><dt style=\"color: cornflowerblue;\">14. Data Governance and Security</dt></b>\n",
    "<dd>\n",
    "Follow best practices for data governance and security, including data anonymization, encryption, and access control, to protect sensitive information."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22a1c4a6-159d-4c80-8636-f10c15877a8b",
   "metadata": {
    "tags": []
   },
   "source": [
    "**Note:** <i>Implementing data governance and security measures is a complex topic involving various tools and methodologies, which is beyond the scope of a code snippet. But it's crucial to handle sensitive data carefully, respecting all necessary regulations (like GDPR, HIPAA, etc.) and implementing necessary security measures (like data encryption, anonymization, and access controls).</i>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (LMG_Kernel)",
   "language": "python",
   "name": "lmg_kernel"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
