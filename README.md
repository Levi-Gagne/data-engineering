# Data Engineering Portfolio

Welcome to my **Data Engineering Portfolio**, a repository showcasing my skills in designing and implementing efficient, scalable data pipelines. This collection highlights my expertise in working with PySpark, Spark SQL, and other tools to solve real-world data engineering challenges.

---

## Repository Overview

This repository is organized into two primary directories:

### **1. Utilities**

Standalone tools that enhance and streamline data engineering workflows.  

- **`Duplicate_Checker_Tool.py`**  
  A PySpark-based tool for identifying duplicate entries in large datasets.  
  - **Features**:
    - Distributed processing for scalability.
    - Interactive, widget-based user interface for usability.
    - Supports filtering for specific value analysis.  
  - **Purpose**: Demonstrates data validation and PySpark proficiency.

- **`ColorConfig.py`**  
  A utility for applying terminal color codes to text output.  
  - **Features**:
    - Predefined and custom text/background colors.
    - Support for text styles like bold and underline.
    - Enhances debugging and log readability.  
  - **Purpose**: Showcases attention to detail and the ability to create developer-friendly tools.

### **2. ETL Scripts**

End-to-end ETL pipelines for batch and incremental data processing using PySpark and Spark SQL.  

- **`CDS_FLEET_OUT_OF_STOCK_LRF.py`**  
  A comprehensive ETL pipeline for processing fleet out-of-stock data and generating actionable KPIs.  
  - **Highlights**:
    - Supports both historical (batch) and incremental workflows.
    - Utilizes Spark SQL for complex data transformations.
    - Implements advanced configurations for optimized PySpark performance.

- **Additional Scripts**:
  - **`OBJ-HISTORICAL.py`**: Batch processing for historical data ingestion.
  - **`OBJ-INCREMENTAL.py`**: Near real-time updates through incremental ingestion.
  - **`Fleet_OOS_Stg-HIS.py`**: Historical fleet out-of-stock staging pipeline.
  - **`Fleet_OOS_Stg-INC.py`**: Incremental fleet out-of-stock staging pipeline.
  - **`CDS_KPI_LMG.py`**: Key Performance Indicator (KPI) calculations for business insights.

---

## Key Skills Demonstrated

- **Data Pipelines**: Designing and implementing ETL workflows for batch and incremental data processing.
- **PySpark & Spark SQL**: Leveraging distributed computing and SQL-based transformations to process large datasets efficiently.
- **Utilities Development**: Building reusable tools to enhance data engineering processes and developer experience.
- **Scalability**: Writing code optimized for high performance and scalable across large datasets.

---

## Purpose of the Repository

This repository is a showcase of my technical skills and problem-solving capabilities as a data engineer. It demonstrates my ability to handle complex data challenges using PySpark, Spark SQL, and other tools while adhering to best practices for maintainability and efficiency.

---

## Contact Information

Feel free to reach out if you'd like to learn more about my work or discuss opportunities:  

- **Name**: Levi Gagne  
- **Email**: [levigagne0@gmail.com](mailto:levigagne0@gmail.com)  
- **Location**: Austin, TX  
- **LinkedIn**: [linkedin.com/in/levi-gagne](https://www.linkedin.com/in/levi-gagne/)  
- **GitHub**: [github.com/Levi-Gagne](https://github.com/Levi-Gagne/)  

---
